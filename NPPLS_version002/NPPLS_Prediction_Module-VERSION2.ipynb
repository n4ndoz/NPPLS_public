{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dihedral Angle Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook comprehends all the steps necessary to parse the inputs from the ProteiNet dataset (link), append physical_chemical descriptors from AAIndex and calculate it's dihedral angles and contact map (distogram on future versions). This is a work in progress and much of what is present here will be changed within the next months. For instance this block only loads a single casp record. To execute it, follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of Libraries\n",
    "\n",
    "The original docker for tensorflow2 doesnt comes with several libraries used throughout this note book. So, pip execution could be broken outside this conteiner and for different versions of the tensorflow dockers available.\n",
    "Eventhough this script could be run on whatever computer that has the requirements met (Tensorflow 1.15 CUDA), be careful when running outside a container. Since I could not verify compatibility with other systems.\n",
    "\n",
    "**The conteiner version and name is:**\n",
    "- TF2.1.0\n",
    "- tensorflow/tensorflow:latest-gpu-py3-jupyter\n",
    "\n",
    "**Note that some of the libraries used on imported modules may be different, all the needed libs are downloaded below and listed on GitHub.**\n",
    "\n",
    "##### Image ran: tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\n",
    "docker pull tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\n",
    "\n",
    "To run the container, one could also make an alias, as so:\n",
    "\n",
    "alias docker_tf='docker run -v /LOCAL/VOLUME/:/tf/CONTAINER_VOLUME -p 8888:8888 --rm --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu-py3-jupyter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run this notebook:\n",
    "1. First of all, download one of the ProteiNet TXT Datasets (this notebook was tested using CASP7's 50 fining);\n",
    "2. Make sure you have the following packages installed:\n",
    ">> - Python 3 <br>\n",
    ">> - Tensorflow <br>\n",
    ">> - Scikit Learn <br>\n",
    ">> - Matplotlib <br>\n",
    ">> - tqdm <br>\n",
    ">> - regex <br>\n",
    ">> (you can download docker and pull/run the above mentioned container) <br>\n",
    "3. Execute the cells in sequence.\n",
    ">> 1. Observe cell description before running. Some of them load files produced on previous steps. So you can continue to explore stuff and skip some cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global and Control Variables\n",
    "<br>\n",
    "The variables listed and declared here will be used to control the entire process. Each of these will be described using comments following the declaration. The parameters written here **are the defaults**. Don't change unless you know exactly what you are doing. For instance, changin _p_number_ could generate a buffer overflow on your computer, and you will end up being mad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import model2\n",
    "import tensorflow as tf\n",
    "from generators import AngleDataGenerator\n",
    "from Utils import Utils as utils\n",
    "#SUpresses TF warnings\n",
    "import numpy as np\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = utils.load_obj('dists_0001.pkl', 'train_70/train_70_xy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 263)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'latest_train_ind2'\n",
    "all_files = listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map all filenames to a given y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, we are using len(all_files)//4 because we have 4 different data types inside the folder\n",
    "# gotta fix this ASAP\n",
    "sequence_dih_map = { 'x_{:04d}.npy'.format(i+1):'y_{:04d}.npy'.format(i+1) for i in np.arange(len(all_files)//4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepares and compiles the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model2.AnglePredictor()\n",
    "model = mod.build_model('tanh','bilstm',[500,46])\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A rough split between 2/3 -1/3 from training data\n",
    "train_ids = dict(list(sequence_dih_map.items())[0:200])\n",
    "valid_ids = dict(list(sequence_dih_map.items())[200:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generators specification\n",
    "train_gen = AngleDataGenerator(train_ids)\n",
    "valid_gen = AngleDataGenerator(valid_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('testing_data/x_0001.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveTestPredCallback(tf.keras.callbacks.Callback):\n",
    "    ' Runs prediction on test set and stores the results inside the test_res folder'\n",
    "    def __init__(self, x_to_pred=None):\n",
    "        self.x_p=x_to_pred\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        print('\\nlalalalalalala')\n",
    "        epoch_pred = self.model.predict(self.x_p)\n",
    "        np.save('teste_pred_{}.npy'.format(epoch),epoch_pred)\n",
    "        print(epoch_pred.shape)\n",
    "        \n",
    "    def on_train_batch_end(self, epoch,logs=None):\n",
    "        print('\\nluululululuulul')\n",
    "\n",
    "mc = SaveTestPredCallback(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 6 steps, validate for 3 steps\n",
      "Epoch 1/2\n",
      "\n",
      "luululululuulul\n",
      "1/6 [====>.........................] - ETA: 2:33 - loss: 0.2916 - accuracy: 0.9070\n",
      "luululululuulul\n",
      "2/6 [=========>....................] - ETA: 1:02 - loss: 0.3761 - accuracy: 0.5692\n",
      "luululululuulul\n",
      "3/6 [==============>...............] - ETA: 31s - loss: 0.3327 - accuracy: 0.4518 \n",
      "luululululuulul\n",
      "4/6 [===================>..........] - ETA: 15s - loss: 0.3248 - accuracy: 0.5780\n",
      "luululululuulul\n",
      "5/6 [========================>.....] - ETA: 6s - loss: 0.3069 - accuracy: 0.6513 \n",
      "luululululuulul\n",
      "\n",
      "lalalalalalala\n",
      "(1, 500, 2)\n",
      "6/6 [==============================] - 43s 7s/step - loss: 0.2933 - accuracy: 0.7012 - val_loss: 0.2498 - val_accuracy: 0.2401\n",
      "Epoch 2/2\n",
      "\n",
      "luululululuulul\n",
      "1/6 [====>.........................] - ETA: 3s - loss: 0.2212 - accuracy: 0.2421\n",
      "luululululuulul\n",
      "2/6 [=========>....................] - ETA: 1s - loss: 0.2588 - accuracy: 0.2651\n",
      "luululululuulul\n",
      "3/6 [==============>...............] - ETA: 1s - loss: 0.2372 - accuracy: 0.2487\n",
      "luululululuulul\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.2379 - accuracy: 0.4233\n",
      "luululululuulul\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2333 - accuracy: 0.5292\n",
      "luululululuulul\n",
      "\n",
      "lalalalalalala\n",
      "(1, 500, 2)\n",
      "6/6 [==============================] - 3s 485ms/step - loss: 0.2253 - accuracy: 0.6000 - val_loss: 0.2444 - val_accuracy: 0.2401\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_gen,\n",
    "          epochs=2,\n",
    "                    callbacks=[mc],\n",
    "          validation_data=valid_gen,\n",
    "          use_multiprocessing=True,\n",
    "         workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.293262558678786, 0.22531992693742117],\n",
       " 'accuracy': [0.7011979, 0.6],\n",
       " 'val_loss': [0.24976701041062674, 0.24439901610215506],\n",
       " 'val_accuracy': [0.2400625, 0.2400625]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some statistics on the model\n",
    "train_losses = history.history['loss']\n",
    "train_acc = history.history['loss']\n",
    "val_losses = history.history['vloss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Save statistics\n",
    "utils.save_batch({'tl':train_losses, 'ta': train_acc, 'vl': val_losses, 'va':val_acc},'',1)\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint(1,6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
