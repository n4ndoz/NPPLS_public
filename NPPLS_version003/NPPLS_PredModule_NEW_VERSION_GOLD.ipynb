{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contained in this notebook are the steps needed to train the neural network models implemented for Protein Structural Features prediction. This one foollows the same rules of the InputPrep Module, but no need for external libs. 27/08/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of Libraries\n",
    "\n",
    "**The container version and name is:**\n",
    "- TF2.1.0\n",
    "- tensorflow/tensorflow:latest-gpu-py3-jupyter\n",
    "\n",
    "##### Image ran: tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\n",
    "docker pull tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\n",
    "\n",
    "To run the container, one could also make an alias, as so:\n",
    "\n",
    "alias docker_tf='docker run -v /LOCAL/VOLUME/:/tf/CONTAINER_VOLUME -p 8888:8888 --rm --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu-py3-jupyter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run this notebook:\n",
    "1. Prepare the files as done by input prep module or open any dump of the data;\n",
    "2. Make sure you have the following packages installed:\n",
    ">> - Python 3 <br>\n",
    ">> - Tensorflow <br>\n",
    ">> - Scikit Learn <br>\n",
    ">> - Matplotlib <br>\n",
    ">> (you can download docker and pull/run the above mentioned container) <br>\n",
    "3. Execute the cells in sequence.\n",
    ">> 1. Observe cell description before running. Some of them load files produced on previous steps. So you can continue to explore stuff and skip some cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerações:\n",
    "NEssa sessão, só há observações sobre o modelo e que podem mudar, assim como notas gerais e etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>1- A rede, do jeito que tá, está funcionando para predição de Theta. mas:\n",
    "<br>1.1- Preciso treinar com uma loss que não considere os ângulos dos loci 0 e 1.\n",
    "<br>2- Preciso de uma loss com pesos também. Ela regride, e calcula o erro dado um peso para uma range. É possível, viável e útil fazer isso?\n",
    "<br>3- Implementar o restart do estado do Adam em caso de stalling de val_loss.\n",
    "<br>3.1- Não façço a mínima ideia de como fazer isso, mas vou ter, talvez, que sobrecarregar o CallBack early stop do keras. Fazer um do zero talvez não seja o melhor approach. Posso ver se dá pra mudar alguma função tipo on_epoch_end\n",
    "<br>4- E se eu colocar uma outra \"mini rede pra predizer a range da distribuição\" do alfabeto? Vai ser viagem\n",
    "<br>5- Preciso refatorar o gerador de dados pra aplicar o ang_type (theta, phi ou PhiPsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padrão ouro de parâmetros (por enquanto)\n",
    "'''\n",
    "Batch Size = 32|53\n",
    "Datasize = 64|53\n",
    "LR0 = 0.001\n",
    "BN after Merge = True\n",
    "RLROP = False\n",
    "opt = Adam\n",
    "Epochs = 140\n",
    "Loss1 = ANG_MAE\n",
    "Loss2 = mae\n",
    "loss3 = RMSE\n",
    "'''\n",
    "# Rede\n",
    "'''\n",
    "config = {'model':'resBiLSTM',\n",
    "            'input_shape': [256,46],\n",
    "         'rnn_width': 250,\n",
    "         'rnn_depth':2,\n",
    "         'rnn_dropout': 0.2,\n",
    "         'ang_mode': 'alphabet',\n",
    "         'alphabet_parms': {'n_alphas': 10, 'n_angles': 1, 'alpha_range': [0.,2*np.pi], 'force_alpha_range': False}}\n",
    "\n",
    "#[0.5,2.7]\n",
    "'''\n",
    "\n",
    "# Melhor padrão foi o Nadam 0003 RLROP, que na vdd usou lr inicial de 0.001 normal. Ou seja, tem que fazer triplicata.\n",
    "# Vamos ver agora com o banco de dados todo. VOu mandar pra JAG e vou treinar lá, shablau.\n",
    "# Agora eu vou programar pra diedrais. Agora o Bicho pega, pq a range é maior, e efetivamente a distribuição é radial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurar por quais descritores usei!! urgente!! mudar os descritores depois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global and Control Variables\n",
    "<br>\n",
    "The variables listed and declared here will be used to control the entire process. Each of these will be described using comments following the declaration. The parameters written here **are the defaults**. Don't change unless you know exactly what you are doing. For instance, changin _p_number_ could generate a buffer overflow on your computer, and you will end up being mad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from generators4 import AngleDataGenerator\n",
    "from Utils import Utils as utils\n",
    "#SUpresses TF warnings\n",
    "import numpy as np\n",
    "import model3\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train_dih/'\n",
    "train_files = listdir(train_dir)\n",
    "val_dir = 'valid_dih/'\n",
    "val_files = listdir(val_dir)\n",
    "#test_dir = 'testing_data/'\n",
    "#test_files = listdir(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map all filenames to a given y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, we are using len(all_files)//4 because we have 4 different data types inside the folder\n",
    "# gotta fix this ASAP\n",
    "seq_dih_map_train = { 'x_{:04d}.npy'.format(i+1):'y_{:04d}.npy'.format(i+1) for i in np.arange(len(train_files)//2)}\n",
    "seq_dih_map_val = { 'x_{:04d}.npy'.format(i+1):'y_{:04d}.npy'.format(i+1) for i in np.arange(71)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = dict(list(seq_dih_map_train.items()))\n",
    "valid_ids = dict(list(seq_dih_map_val.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generators specification\n",
    "train_gen = AngleDataGenerator(train_ids,16,data_dir='train_dih/',ang_type='dih')\n",
    "valid_gen = AngleDataGenerator(valid_ids,53,data_dir='valid_dih/',ang_type='dih')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepares and compiles the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model':'resBiLSTM',\n",
    "            'input_shape': [256,46],\n",
    "         'rnn_width': 250,\n",
    "         'rnn_depth':2,\n",
    "         'rnn_dropout': 0.2,\n",
    "         'ang_mode': 'alphabet',\n",
    "         'alphabet_parms': {'n_alphas': 10, 'n_angles': 1, 'alpha_range': [0.,2*np.pi], 'force_alpha_range': False}}\n",
    "\n",
    "#[0.5,2.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mod_lib = model3.AnglePredictor(config)\n",
    "model = mod_lib.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Alternative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization, Masking, Lambda, LSTM, Dense, RepeatVector, TimeDistributed,Conv1D, Input, add\n",
    "from layer_utils_3 import Alphabet\n",
    "import os\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN, tcn_full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.load('train_70_dih/x_0001.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import model_teste3 as MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = MODEL.AnglePredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(256,46))\n",
    "outputs = model_(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 46)]         0         \n",
      "_________________________________________________________________\n",
      "angle_predictor (AnglePredic (None, 256, 2)            147646    \n",
      "=================================================================\n",
      "Total params: 147,646\n",
      "Trainable params: 143,550\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_zero=0\n",
    "for r in a[0]:\n",
    "    if np.all(r == 0):\n",
    "        count_zero+=1\n",
    "print(count_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing TCN Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Angularization():\n",
    "    '''\n",
    "    Angularization Class\n",
    "    In this class are contained the means to process the core model\n",
    "    outputs into angles.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def regression_tanh(inputs,n_angles):\n",
    "        x = inputs\n",
    "        x = TimeDistributed(Dense(n_angles,activation='tanh'))(x)\n",
    "        x = Lambda(lambda x: x * 3.141592653589793,name='lambda_tanh')(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def alphabet(inputs,n_angles, n_alphas, alpha_range):\n",
    "        x = inputs\n",
    "        x = Alphabet(n_alphas, n_angles)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreModels():\n",
    "    @staticmethod\n",
    "    def ResLSTM(inputs, rnn_width=180, rnn_depth=4, rnn_dropout=0.8, bn=True):\n",
    "        x = inputs\n",
    "        for i in range(rnn_depth):\n",
    "            #i < rnn_depth - 1\n",
    "            x_rnn = Bidirectional(LSTM(rnn_width, recurrent_dropout=rnn_dropout,\n",
    "                                       dropout=rnn_dropout, return_sequences=True))(x)\n",
    "            if i > 0:\n",
    "                x = add([x, x_rnn])\n",
    "                x = BatchNormalization()(x) if bn else x\n",
    "            else:\n",
    "                x = x_rnn\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def BiLSTM(inputs):\n",
    "        x=inputs\n",
    "        rnn_width=100 \n",
    "        rnn_depth=2\n",
    "        rnn_dropout=0.2\n",
    "        for cell_count in range(rnn_depth):\n",
    "            x = Bidirectional(LSTM(rnn_width, recurrent_dropout=rnn_dropout, dropout=rnn_dropout, return_sequences=True))(x)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def StackedTCN(inputs, stacks=[64,32], nb_stacks=2, kernel_sizes=[2,2]):\n",
    "        '''\n",
    "        Stacked Temporal Convolutional Network\n",
    "        COnsists of stacks of TCNs, as implemented by Philipp Remy at\n",
    "        https://github.com/philipperemy/keras-tcn. Parameters are:\n",
    "        stacks: (list) are the numbers of filters\n",
    "        nb_stacks: (int) the number of stacks of residual blocks on the TCN layer (lib stuff)\n",
    "        kernel_sizes: (list) list of kernel sizes for the TCN stacks.\n",
    "        \n",
    "        '''\n",
    "        x = inputs\n",
    "        for stack in stacks:\n",
    "            x = TCN(nb_filters=stack, kernel_size=2, nb_stacks=2, dilations=[1, 2, 4, 8, 16],\n",
    "            padding='causal', use_skip_connections=True, dropout_rate=0.2, return_sequences=True,\n",
    "            activation='elu', kernel_initializer='he_normal', use_batch_norm=True)(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input((250,46))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = CoreModels.StackedTCN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = Angularization.alphabet(o,2,10,[-np.pi,np.pi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(inputs=i,outputs=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = m.predict(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet class almost done\n",
    "# What has to be done (by order of priority)\n",
    "# 1- Masking propagation (support?) through TCN layer\n",
    "# 2- Training loop\n",
    "# 3- Angle Vectorization support for Alphabet Layer\n",
    "# 4- Geometric Layer\n",
    "\n",
    "class Alphabet(tf.keras.layers.Layer):\n",
    "    def __init__(self,n_alphas=10, n_angles=2, alpha_range = [0,1],\n",
    "                 force_alpha_range=True, name='alphabet_layer', **kwargs):\n",
    "        \n",
    "        '''\n",
    "        Alphabet Layer:\n",
    "        This layer consists of a Densely connected network followed by a linear/angular regression operation.\n",
    "        The main purpose is to convert the information gathered from the core models into a probability distribution\n",
    "        via softmax function. By doing this we then perform the \"angular regression\" and achieve angle prediction.\n",
    "        \n",
    "        Angular prediction comes in two flavors: angular regression (as of now) and vectorized (later on).\n",
    "        Layer parameters:\n",
    "        n_alphas: (int) number of angle alphabet units to consider\n",
    "        n_angles: (int) number of angles to predict (2: [phi, psi], 3: [phi, psi, omega])\n",
    "        alpha_range: (list,tuple) the initialization range for the alphabet.\n",
    "        force_alpha_range: (bool) wether or not to force the alphabet to stay on the initial range.\n",
    "        \n",
    "        Just a disclaimer:\n",
    "        It doest support \"\"\"\" masking \"\"\"\" to some extent. It means that the layer receives a 'mask' tensor\n",
    "        propagated from previous layers (originally from Masking) and casts it to float32. Followed by a \n",
    "        tensor product between float_mask and output from Alphabet. The idea is that, by passing the mask and\n",
    "        performing the product, BP algorithm will gradually propagatethe importance of sequence length.\n",
    "        I have no clues whatsoever if this will work as intended, but  it is a try. \n",
    "        '''\n",
    "        \n",
    "        super(Alphabet, self).__init__(**kwargs)\n",
    "        self.n_angles = n_angles\n",
    "        self.n_alphas = n_alphas\n",
    "        #self.alphabet = alpabet\n",
    "        self.alpha_range=alpha_range\n",
    "        #assert type(self.alpha_range) in [list,tuple], \"alpha_range must be a tuple or list of values.\"\n",
    "        \n",
    "        self.force_alpha_range = force_alpha_range\n",
    "        \n",
    "    def init_alphabet(self):\n",
    "        '''\n",
    "        Initializes the Angles Alphabet Matrix\n",
    "        returns:\n",
    "            Alphabet: matrix of floats of shape (n_alphas, n_angles)\n",
    "        '''\n",
    "        return tf.random.uniform(shape=(self.n_alphas, self.n_angles),\n",
    "                                     minval=self.alpha_range[0], maxval=self.alpha_range[1],\n",
    "                                     dtype='float32')\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        # Time step masks must be the same for each input.\n",
    "        # This is because the mask for an RNN is of size [batch, time_steps, 1],\n",
    "        # and specifies which time steps should be skipped, and a time step\n",
    "        # must be skipped for all inputs.\n",
    "        # TODO(scottzhu): Should we accept multiple different masks?\n",
    "        '''\n",
    "        mask = nest.flatten(mask)[0]\n",
    "        output_mask = mask if self.return_sequences else None\n",
    "        if self.return_state:\n",
    "            state_mask = [None for _ in self.states]\n",
    "            return [output_mask] + state_mask\n",
    "        else:\n",
    "            return output_mask\n",
    "        '''\n",
    "        if mask is not None:\n",
    "            return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        # overiding get_config function\n",
    "        # to enable layer serialization\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "                    'n_alphas': self.n_alphas, \n",
    "                    'n_angles': self.n_angles,\n",
    "                    'alpha_range': self.alpha_range,\n",
    "                    'force_alpha_range': self.force_alpha_range\n",
    "                    })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.dense_unit = TimeDistributed(tf.keras.layers.Dense(self.n_alphas,\n",
    "                                                activation='relu',\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=\"glorot_uniform\",\n",
    "                                                bias_initializer=\"zeros\"))\n",
    "        \n",
    "        self.alphabet = tf.Variable(self.init_alphabet(), trainable = True,name='alphabet')\n",
    "        \n",
    "    def call(self, input, mask=None):\n",
    "        '''\n",
    "        This method is called everytime the layer is invoked\n",
    "        by the model. Actually what it does is passing the input by a \n",
    "        TimeDistributed Dense and then softmax it along the last axis and\n",
    "        perform the weighted average (\"angular regression\") over the\n",
    "        alphabet array. Which, in turn, is also a learnable parameter.4\n",
    "        '''\n",
    "        outputs = self.dense_unit(input)\n",
    "        \n",
    "        outputs = tf.keras.activations.softmax(outputs)\n",
    "        outputs = tf.einsum('ij,kbi->kbj', self.alphabet, outputs)\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, \"float32\")\n",
    "            outputs = outputs * mask[..., tf.newaxis]\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, timesteps, input_dim = 1, 256, 46\n",
    "n_alphas, n_angles = 10, 2\n",
    "i = Input((timesteps, input_dim))\n",
    "mask = Masking(0)(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Core Learning Layer\n",
    "# Gotta change the Sequence Training Data Generator later\n",
    "# Change TCN layer to compute a mask. Actually need to change both the TCN and the Residual Block\n",
    "o = TCN(nb_filters=64, kernel_size=2, nb_stacks=2, dilations=[1, 2, 4, 8, 16],\n",
    "        padding='causal', use_skip_connections=True, dropout_rate=0.2, return_sequences=True,\n",
    "        activation='elu', kernel_initializer='he_normal', use_batch_norm=True)(mask)  # The TCN layers are here.\n",
    "o = TCN(nb_filters=32, kernel_size=2, nb_stacks=2, dilations=[1, 2, 4, 8],\n",
    "        padding='causal', use_skip_connections=False, dropout_rate=0.1, return_sequences=True,\n",
    "        activation='elu', kernel_initializer='he_normal', use_batch_norm=True)(o)  # The TCN layers are here.\n",
    "############################################################################\n",
    "# Needs a masking multiplication layer just over here                      \n",
    "# The masking multiplication layer consists of an einsum layer, just that\n",
    "# It will multiply the TCN results by the binary mask received on input.\n",
    "############################################################################\n",
    "\n",
    "#o = LSTM(64,return_sequences=True)(mask)\n",
    "\n",
    "# Logit construction layer\n",
    "o = Alphabet(10,2)(o)\n",
    "\n",
    "#o = TimeDistributed(Dense(n_clusters))(o)\n",
    "#o = Tester(2)(o)\n",
    "m = Model(inputs=[i], outputs=[o])\n",
    "m.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Alphabet\n",
    "O novo layer Alphabet será definido a partir da implementação de m3h0w as instruções do layer são como se segue:<br>\n",
    "0- No início do treinamento: inicia os clusters na range $[l,u]$. <br>\n",
    "1- Recebe um tensor do modelo core (TCN, LSTM, etc) na forma (batch, Seq_Len, Features) <br>\n",
    "2- Passa esse tensor por um dense + softmax pra pegar **logits** com shape $(batch, seqLen, logits)$ <br>\n",
    "3- Clipa em clusters em $[-\\pi,\\pi]$. <br>\n",
    "4- Einsum de **logits** com clusters pra pegar **angles** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a masking layer to work properly\n",
    "# \n",
    "\n",
    "class Alphabet(tf.keras.layers.Layer):\n",
    "    def __init__(self,n_alphas=20, n_angles=2, alpha_range = [-np.pi,np.pi],\n",
    "                 force_alpha_range=True, name='alphabet_layer', **kwargs):\n",
    "        #super(Alphabet, self).__init__()\n",
    "        \n",
    "        self.n_alphas = n_alphas\n",
    "        self.n_angles = n_angles\n",
    "        self.alpha_range = alpha_range\n",
    "        # the values for the dense unit will be changed later on\n",
    "        \n",
    "        self.force_alpha_range=force_alpha_range\n",
    "        super(Alphabet).__init__(**kwargs)\n",
    "    def init_alphabet(self):\n",
    "        '''\n",
    "        Initializes the Angles Alphabet Tensor\n",
    "        returns:\n",
    "            Alphabet: tensor of floats of shape (n_alphas, n_angles)\n",
    "        '''\n",
    "        return tf.random.uniform(shape=(self.n_alphas, self.n_angles),\n",
    "                                     minval=self.alpha_range[0], maxval = self.alpha_range[1],\n",
    "                                     dtype='float32')\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        self.alphabet = tf.Variable(self.init_alphabet(), trainable = True,name='alphabet')\n",
    "        self.dense_unit = tf.keras.layers.Dense(n_alphas,\n",
    "                                                activation='elu',\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=\"glorot_uniform\",\n",
    "                                                bias_initializer=\"zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        #self.state_size = [tf.TensorShape([units])]\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        print(type(input_shape))\n",
    "        self.kernel = self.add_weight(shape=(input_shape[0][-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = K.dot(inputs[0], self.kernel)\n",
    "        output = h + K.dot(prev_output, self.recurrent_kernel)\n",
    "        return output, [output]   \n",
    "\n",
    "# Let's use this cell in a RNN layer:\n",
    "cell = MinimalRNNCell(32)\n",
    "input_1 = tf.keras.Input((None, 5))\n",
    "input_2 = tf.keras.Input((None, 5))\n",
    "layer = RNN(cell)\n",
    "y = layer((input_1, input_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform(shape=(10, 2),\n",
    "                                     minval=-np.pi, maxval = np.pi,\n",
    "                                     dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.predict(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_zero=0\n",
    "for r in pred[0]:\n",
    "    if np.all(r == 0):\n",
    "        count_zero+=1\n",
    "print(count_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'testing_data/'\n",
    "test_files = listdir(test_dir)\n",
    "seq_dih_map_test = { 'x_{:04d}.npy'.format(i+1):'y_{:04d}.npy'.format(i+1) for i in np.arange(len(test_files)//4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is being tested as of now. But will become part of the workflow asap\n",
    "def angular_MAE(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(tf.atan2(tf.sin(y_true - y_pred), tf.cos(y_true - y_pred))), axis=-1)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_pred[:,3:,:] - y_true[:,3:,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SaveTestPredCallback(tf.keras.callbacks.Callback):\n",
    "    ' Runs prediction on test set and stores the results inside the test_res folder'\n",
    "    def __init__(self, x_to_pred=None):\n",
    "        self.x_p=x_to_pred\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        \n",
    "        epoch_pred = self.model.predict_on_batch(self.x_p)\n",
    "        np.save('testing_data/teste_pred_{}.npy'.format(epoch),epoch_pred)\n",
    "        del epoch_pred\n",
    "\n",
    "\n",
    "RLROP = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='min',\n",
    "    min_delta=0.1, cooldown=10, min_lr=0.000000011)\n",
    "        \n",
    "import resource\n",
    "class MemoryCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        print('\\n{}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss*(1/1024*1024)))\n",
    "\n",
    "class FollowOPT(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        opt_parms = [self.model.optimizer.lr.numpy(), self.model.optimizer.iterations.numpy(), self.model.optimizer.decay.numpy()]\n",
    "        print('\\nLR: {} | Iter: {} | Decay: {}\\n'.format(*opt_parms))\n",
    "#class RestartOpt(tf.keras)\n",
    "# keep this commented for now\n",
    "# \n",
    "# tensorboard callback\n",
    "#from time import datetime\n",
    "#logs = \"logs/lr_001_RLROP_ang_mae_batchNorm_bottleneck\" #+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/TCN_run1_alphabet_dih_07092020\"\n",
    "fo = FollowOPT()\n",
    "tbcb = tf.keras.callbacks.TensorBoard(log_dir = log_dir,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                write_images=False)#,\n",
    "                                                 #profile_batch = '500,520')\n",
    "        \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
    "#mc = SaveTestPredCallback(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 46)]         0         \n",
      "_________________________________________________________________\n",
      "angle_predictor (AnglePredic (None, 256, 2)            147646    \n",
      "=================================================================\n",
      "Total params: 147,646\n",
      "Trainable params: 143,550\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=angular_MAE, optimizer=opt, metrics=['mae',rmse,'mse'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.5603 - mae: 0.6714 - rmse: 1.3124 - mse: 1.7666\n",
      "LR: 9.999999747378752e-05 | Iter: 394 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 73s 185ms/step - loss: 0.5603 - mae: 0.6714 - rmse: 1.3124 - mse: 1.7666 - val_loss: 0.4592 - val_mae: 0.5099 - val_rmse: 1.1142 - val_mse: 1.2723\n",
      "Epoch 2/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.5119 - mae: 0.6013 - rmse: 1.2443 - mse: 1.5859\n",
      "LR: 9.999999747378752e-05 | Iter: 788 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 21s 54ms/step - loss: 0.5119 - mae: 0.6013 - rmse: 1.2443 - mse: 1.5859 - val_loss: 0.4366 - val_mae: 0.4677 - val_rmse: 1.0452 - val_mse: 1.1243\n",
      "Epoch 3/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4996 - mae: 0.5501 - rmse: 1.1424 - mse: 1.3418\n",
      "LR: 9.999999747378752e-05 | Iter: 1182 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4996 - mae: 0.5501 - rmse: 1.1424 - mse: 1.3418 - val_loss: 0.4307 - val_mae: 0.4517 - val_rmse: 0.9979 - val_mse: 1.0281\n",
      "Epoch 4/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4906 - mae: 0.5219 - rmse: 1.0801 - mse: 1.2032\n",
      "LR: 9.999999747378752e-05 | Iter: 1576 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4906 - mae: 0.5219 - rmse: 1.0801 - mse: 1.2032 - val_loss: 0.4333 - val_mae: 0.4550 - val_rmse: 1.0104 - val_mse: 1.0535\n",
      "Epoch 5/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4845 - mae: 0.5075 - rmse: 1.0474 - mse: 1.1322\n",
      "LR: 9.999999747378752e-05 | Iter: 1970 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4845 - mae: 0.5075 - rmse: 1.0474 - mse: 1.1322 - val_loss: 0.4280 - val_mae: 0.4501 - val_rmse: 1.0086 - val_mse: 1.0481\n",
      "Epoch 6/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4801 - mae: 0.4976 - rmse: 1.0215 - mse: 1.0778\n",
      "LR: 9.999999747378752e-05 | Iter: 2364 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4801 - mae: 0.4976 - rmse: 1.0215 - mse: 1.0778 - val_loss: 0.4132 - val_mae: 0.4308 - val_rmse: 0.9662 - val_mse: 0.9644\n",
      "Epoch 7/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4762 - mae: 0.4889 - rmse: 0.9944 - mse: 1.0218\n",
      "LR: 9.999999747378752e-05 | Iter: 2758 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4762 - mae: 0.4889 - rmse: 0.9944 - mse: 1.0218 - val_loss: 0.3832 - val_mae: 0.4003 - val_rmse: 0.9270 - val_mse: 0.8875\n",
      "Epoch 8/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4730 - mae: 0.4829 - rmse: 0.9762 - mse: 0.9837\n",
      "LR: 9.999999747378752e-05 | Iter: 3152 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4730 - mae: 0.4829 - rmse: 0.9762 - mse: 0.9837 - val_loss: 0.3558 - val_mae: 0.3639 - val_rmse: 0.8560 - val_mse: 0.7601\n",
      "Epoch 9/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4697 - mae: 0.4785 - rmse: 0.9647 - mse: 0.9587\n",
      "LR: 9.999999747378752e-05 | Iter: 3546 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4697 - mae: 0.4785 - rmse: 0.9647 - mse: 0.9587 - val_loss: 0.4226 - val_mae: 0.4373 - val_rmse: 0.9617 - val_mse: 0.9491\n",
      "Epoch 10/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4666 - mae: 0.4746 - rmse: 0.9541 - mse: 0.9370\n",
      "LR: 9.999999747378752e-05 | Iter: 3940 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4666 - mae: 0.4746 - rmse: 0.9541 - mse: 0.9370 - val_loss: 0.4101 - val_mae: 0.4200 - val_rmse: 0.9360 - val_mse: 0.8968\n",
      "Epoch 11/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4636 - mae: 0.4715 - rmse: 0.9465 - mse: 0.9212\n",
      "LR: 9.999999747378752e-05 | Iter: 4334 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4636 - mae: 0.4715 - rmse: 0.9465 - mse: 0.9212 - val_loss: 0.4117 - val_mae: 0.4225 - val_rmse: 0.9397 - val_mse: 0.9053\n",
      "Epoch 12/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4606 - mae: 0.4685 - rmse: 0.9393 - mse: 0.9074\n",
      "LR: 9.999999747378752e-05 | Iter: 4728 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4606 - mae: 0.4685 - rmse: 0.9393 - mse: 0.9074 - val_loss: 0.4005 - val_mae: 0.4109 - val_rmse: 0.9193 - val_mse: 0.8652\n",
      "Epoch 13/140\n",
      "394/394 [==============================] - ETA: 0s - loss: 0.4573 - mae: 0.4654 - rmse: 0.9336 - mse: 0.8961\n",
      "LR: 9.999999747378752e-05 | Iter: 5122 | Decay: 0.0\n",
      "\n",
      "394/394 [==============================] - 22s 55ms/step - loss: 0.4573 - mae: 0.4654 - rmse: 0.9336 - mse: 0.8961 - val_loss: 0.3800 - val_mae: 0.3963 - val_rmse: 0.9141 - val_mse: 0.8583\n",
      "Epoch 14/140\n",
      "154/394 [==========>...................] - ETA: 13s - loss: 0.4502 - mae: 0.4574 - rmse: 0.9192 - mse: 0.8692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-258:\n",
      "Process Keras_worker_ForkPoolWorker-254:\n",
      "Process Keras_worker_ForkPoolWorker-257:\n",
      "Process Keras_worker_ForkPoolWorker-255:\n",
      "Process Keras_worker_ForkPoolWorker-253:\n",
      "Process Keras_worker_ForkPoolWorker-256:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4f8499ca07e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m      workers = 6)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x=train_gen,\n",
    "      epochs=100, callbacks=[tbcb, fo],#, RLROP],\n",
    "      validation_data=valid_gen,   \n",
    "      use_multiprocessing=True,\n",
    "     workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-1].layer.alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "#%tensorboard --logdir jag_train1/results_jag/logs/ --host 0.0.0.0 --port 6015\n",
    "%tensorboard --logdir logs/ --host 0.0.0.0 --port 6014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_obj(history.history,'history_padrao_ouro','./')\n",
    "model.save('padrao_ouro.h5')\n",
    "model.save_weights('padrao_ouro.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_obj(h,'history_bilstm15_64cells_no_callbacks','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = np.load('train_70_zmat/y_0001.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Angle Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rose_plot(ax, angles, bins=40, density=False, offset=0, lab_unit=\"degrees\",\n",
    "              start_zero=True, **param_dict):\n",
    "    \"\"\"\n",
    "    Plot polar histogram of angles on ax. ax must have been created using\n",
    "    subplot_kw=dict(projection='polar'). Angles are expected in radians.\n",
    "    \"\"\"\n",
    "    # Wrap angles to [-pi, pi)\n",
    "    angles = (angles + np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "    # Set bins symetrically around zero\n",
    "    if start_zero:\n",
    "        # To have a bin edge at zero use an even number of bins\n",
    "        if bins % 2:\n",
    "            bins += 1\n",
    "        bins = np.linspace(-np.pi, np.pi, num=bins+1)\n",
    "\n",
    "    # Bin data and record counts\n",
    "    count, bin = np.histogram(angles, bins=bins)\n",
    "\n",
    "    # Compute width of each bin\n",
    "    widths = np.diff(bin)\n",
    "\n",
    "    # By default plot density (frequency potentially misleading)\n",
    "    if density is None or density is True:\n",
    "        # Area to assign each bin\n",
    "        area = count / angles.size\n",
    "        # Calculate corresponding bin radius\n",
    "        radius = (area / np.pi)**.5\n",
    "    else:\n",
    "        radius = count\n",
    "\n",
    "    # Plot data on ax\n",
    "    ax.bar(bin[:-1], radius, zorder=1, align='edge', width=widths,\n",
    "           edgecolor='C0', fill=False, linewidth=1)\n",
    "\n",
    "    # Set the direction of the zero angle\n",
    "    ax.set_theta_offset(offset)\n",
    "\n",
    "    # Remove ylabels, they are mostly obstructive and not informative\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    if lab_unit == \"radians\":\n",
    "        label = ['$0$', r'$\\pi/4$', r'$\\pi/2$', r'$3\\pi/4$',\n",
    "                  r'$\\pi$', r'$5\\pi/4$', r'$3\\pi/2$', r'$7\\pi/4$']\n",
    "        ax.set_xticklabels(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "SMALL = 10\n",
    "MEDIUM = 14\n",
    "BIG = 22\n",
    "\n",
    "plt.rc('font', size=SMALL)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIG)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_to_pdb2(coords_sets):\n",
    "    \"\"\" Write a chain to a PDB file \"\"\"\n",
    "    pdb_lines = ''\n",
    "    counter = 1\n",
    "    atom_types = [' N  ', ' CA ', ' C ']\n",
    "    elements = ['N', 'C', 'C',]\n",
    "    element_dict = dict(zip(atom_types, elements))\n",
    "    # N CA C O SCOM\n",
    "    # example = 'ATOM      1  N   HIS A 218      11.361  61.787  24.572  1.00 38.92           N  '\n",
    "    pdb_lines = []\n",
    "    for i in range(coords_sets.shape[0]): \n",
    "        #print(coords_sets[i].shape)\n",
    "        for atom_type, X in zip(atom_types, coords_sets[i]):\n",
    "            # N CA C O SCOM\n",
    "            #  1 -  6        Record name   \"ATOM  \"\n",
    "            line = 'ATOM  '\n",
    "            # 7 - 11        Integer       serial       Atom  serial\n",
    "            # number.\n",
    "            line = line + str(counter).rjust(5) + ' '\n",
    "            # 13 - 16        Atom          name         Atom name.\n",
    "            line = line + atom_type\n",
    "            # 17             Character     altLoc       Alternate\n",
    "            # location indicator.\n",
    "            line = line + ' '\n",
    "            # 18 - 20        Residue name  resName      Residue name.\n",
    "            line = line + 'ALA '\n",
    "            # 22             Character     chainID      Chain\n",
    "            # identifier.\n",
    "            line = line + 'A'\n",
    "            # 23 - 26        Integer       resSeq       Residue\n",
    "            # sequence number.\n",
    "            line = line + str(i + 1).rjust(4)\n",
    "            # 27             AChar         iCode        Code for\n",
    "            # insertion of residues.\n",
    "            line = line + ' ' + '   '\n",
    "            # 31 - 38        Real(8.3)     x            Orthogonal\n",
    "            # coordinates for X in Angstroms.\n",
    "            line = line + ('%.3f' % X[0]).rjust(8)\n",
    "            # 39 - 46        Real(8.3)     y            Orthogonal\n",
    "            # coordinates for Y in Angstroms.\n",
    "            line = line + ('%.3f' % X[1]).rjust(8)\n",
    "            # 47 - 54        Real(8.3)     z            Orthogonal\n",
    "            # coordinates for Z in Angstroms.\n",
    "            line = line + ('%.3f' % X[2]).rjust(8)\n",
    "            # 55 - 60        Real(6.2)     occupancy    Occupancy.\n",
    "            line = line + '  1.00'\n",
    "            # 61 - 66        Real(6.2)     tempFactor   Temperature\n",
    "            # factor.\n",
    "            line = line + ' 12.00'\n",
    "            # 77 - 78        LString(2)    element      Element symbol,\n",
    "            # right-justified.\n",
    "            line = line + '          ' + element_dict[atom_type]\n",
    "            # 79 - 80        LString(2)    charge       Charge  on the\n",
    "            # atom.\n",
    "            pdb_lines.append(line)\n",
    "            counter = counter + 1\n",
    "    return pdb_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
