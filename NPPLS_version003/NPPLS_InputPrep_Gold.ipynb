{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPPLS Data Preparation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook comprehends all the steps necessary to parse the inputs from the ProteiNet dataset (link), append physical_chemical descriptors from AAIndex and calculate it's ZMatrix, dihedral angles and distogram (currently Ca, planning on expanding to Cbeta but need answers from ProteinNet's creator). This is a work in progress and much of what is present here will be changed within the next months. \n",
    "<br>**Essa que tÃ¡ valendo!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of Libraries\n",
    "\n",
    "The original docker for tensorflow2 doesnt comes with several libraries used throughout this note book. So, pip execution could be broken outside this conteiner and for different versions of the tensorflow dockers available.\n",
    "Eventhough this script could be run on whatever computer that has the requirements met (Tensorflow 2.1.0 CUDA), be careful when running outside a container, since I could not verify compatibility with other systems.\n",
    "\n",
    "**The conteiner version and name is:**\n",
    "- TF 2.2.0\n",
    "- tensorflow/tensorflow:latest-gpu-py3-jupyter\n",
    "\n",
    "**Note that some of the libraries used on imported modules may be different, all the needed libs are downloaded below and listed on GitHub.**\n",
    "\n",
    "##### Image ran: tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\n",
    "docker pull tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\n",
    "\n",
    "To run the container, one could also make an alias, as so:\n",
    "\n",
    "alias docker_tf='docker run -v /LOCAL/VOLUME/:/tf/CONTAINER_VOLUME -p 8888:8888 --rm --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu-py3-jupyter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run this notebook:\n",
    "1. First of all, download one of the ProteiNet TXT Datasets (this notebook was tested using CASP7's 50 fining);\n",
    "2. Make sure you have the following packages installed:\n",
    ">> - Python 3 <br>\n",
    ">> - Tensorflow <br>\n",
    ">> - Scikit Learn <br>\n",
    ">> - Matplotlib <br>\n",
    ">> - tqdm <br>\n",
    ">> - regex <br>\n",
    ">> (you can download docker and pull/run the above mentioned container) <br>\n",
    "3. Execute the cells in sequence.\n",
    ">> 1. Observe cell description before running. Some of them load files produced on previous steps. So you can continue to explore stuff and skip some cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global and Control Variables\n",
    "<br>\n",
    "The variables listed and declared here will be used to control the entire process. Each of these will be described using comments following the declaration. The parameters written here **are the defaults**. Don't change unless you know exactly what you are doing. For instance, changin _p_number_ could generate a buffer overflow on your computer, and you will end up being mad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to install Package: tqdm\n",
      "Trying to install Package: scipy\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function for installation of libraries via pip\n",
    "def install_pkg(package):\n",
    "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "pkgs = ['tqdm','scipy']\n",
    "\n",
    "for package in pkgs:\n",
    "    try:\n",
    "        import package\n",
    "    except ImportError:\n",
    "        print('Trying to install Package: {}'.format(package))\n",
    "        install_pkg(package)\n",
    "\n",
    "# Import sub-block\n",
    "# Takes care of already installed libraries on the container\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from Utils import Utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPrep Class\n",
    "\n",
    "This class consists of a series of methods applied to the data preparation pipeline. The first step is to import the lib. Here we import as data_prep_lib out of plain lazyness. One day I will really change the name os this class. God knows I will.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import data_prep_test2 as data_prep_lib\n",
    "import datapreplib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing DIH data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dihedral Angle files Prep**<br>\n",
    "Outputs:<br>\n",
    "1- Sequence and descriptors;<br>\n",
    "2- $\\phi$ and $\\psi$ of each protein;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output files location\n",
    "data_files = {'train':['/tf/fernando/storage/model1/Model1_pipeline/casp7/training_70','train_70_dih'],#,\n",
    "              'valid':['/tf/fernando/storage/model1/Model1_pipeline/casp7/validation','valid_dih'],\n",
    "               'test':['/tf/fernando/storage/model1/Model1_pipeline/casp7/testing','testing_data_dih']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation Routine\n",
    "for k in data_files.keys():\n",
    "    print('Preparing file * {} * | storing at * {} *'.format(data_files[k][0],data_files[k][1]))\n",
    "    data_prep = data_prep_lib.Data_Prep_Pipeline(input_file=data_files[k][0])\n",
    "    data_prep.prep_data(mode='dih',save_dir=data_files[k][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dist files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance routines preparation**<br>\n",
    "Which includes:<br>\n",
    "1- Sequence and descritors; <br>\n",
    "2- Inter-$C_\\alpha$ distance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination of the individual protein files\n",
    "data_files = {'train':['/tf/fernando/storage/model1/Model1_pipeline/casp7/training_70','train_70_dist'],#,\n",
    "              'valid':['/tf/fernando/storage/model1/Model1_pipeline/casp7/validation','valid_dist'],\n",
    "               'test':['/tf/fernando/storage/model1/Model1_pipeline/casp7/testing','testing_data_dist']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation Routine\n",
    "for k in data_files.keys():\n",
    "    print('Preparing file * {} * | storing at * {} *'.format(data_files[k][0],data_files[k][1]))\n",
    "    data_prep = data_prep_lib.Data_Prep_Pipeline(input_file=data_files[k][0])\n",
    "    data_prep.prep_data(mode='dist',save_dir=data_files[k][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing ZMat files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-Matrix preparation**<br>\n",
    "The outputs:<br>\n",
    "1- Sequence and descritors<br>\n",
    "2- Zmatrix reppresentations for roteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'train':['/tf/fernando/storage/model1/Model1_pipeline/casp7/training_70','train_70_zmat2']}#,\n",
    "              'valid':['/tf/fernando/storage/model1/Model1_pipeline/casp7/validation','valid_zmat'],\n",
    "              'test':['/tf/fernando/storage/model1/Model1_pipeline/casp7/testing','testing_data_zmat']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing stuff, will be deleted soon (24/10/2020)\n",
    "'''\n",
    "data_files = {'train':['/tf/fernando/storage/model1/Model1_pipeline/casp7/training_70','train_tert_final'],\n",
    "              'valid':['/tf/fernando/storage/model1/Model1_pipeline/casp7/validation','valid_tert_final'],\n",
    "              'test':['/tf/fernando/storage/model1/Model1_pipeline/casp7/testing','testing_data_tert_final']}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing file * /tf/fernando/storage/model1/Model1_pipeline/casp7/training_70 * | storing at * train_tert_final *\n",
      "# Done Processing data.hes\n",
      "Preparing file * /tf/fernando/storage/model1/Model1_pipeline/casp7/validation * | storing at * valid_tert_final *\n",
      "# Done Processing data.s\n",
      "Preparing file * /tf/fernando/storage/model1/Model1_pipeline/casp7/testing * | storing at * testing_data_tert_final *\n",
      "# Done Processing data.s\n"
     ]
    }
   ],
   "source": [
    "for k in data_files.keys():\n",
    "    print('Preparing file * {} * | storing at * {} *'.format(data_files[k][0],data_files[k][1]))\n",
    "    data_prep = data_prep_lib.Data_Prep_Pipeline(input_file=data_files[k][0])\n",
    "    data_prep.prep_data(mode='zmat',save_dir=data_files[k][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
